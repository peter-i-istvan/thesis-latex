\chapter{The Transformer architecture in Object Detection}

The Transformer architecture originally emerged in the field of Natural Language Processing (NLP), in translation tasks (more generally called \textit{sequence transduction} tasks) as attention-based approaches were gaining popularity. In the \texbf{Attention is all you need} paper